{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Capstone – Option 4: Prédiction du risque de réadmission hospitalière (classification)\n",
        "\n",
        "Objectif: utiliser un dataset public (Diabetes 130-US hospitals) pour prédire si un patient sera réadmis rapidement, et produire un mini-rapport.\n",
        "\n",
        "**Ce notebook couvre les 3 exigences:**\n",
        "- **Data Processing**: nettoyage + EDA + visualisations\n",
        "- **Machine Learning**: classification + comparaison de modèles + validation\n",
        "- **Generative AI**: génération d’un résumé exécutif (et option RAG)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1) Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, ConfusionMatrixDisplay,\n",
        "    roc_auc_score, RocCurveDisplay\n",
        ")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Charger le dataset\n",
        "Deux options:\n",
        "1) **Local**: télécharge `diabetic_data.csv` depuis UCI/Kaggle et mets le fichier dans le même dossier.\n",
        "2) **OpenML** (si internet autorisé dans ton environnement): `fetch_openml`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Option A (recommandée): fichier local\n",
        "DATA_PATH = 'diabetic_data.csv'  # <-- ajuste si besoin\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 3) Data Processing – audit qualité rapide\n",
        "display(df.info())\n",
        "display(df.isna().sum().sort_values(ascending=False).head(20))\n",
        "display(df.describe(include='all').T.head(20))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Définir la cible\n",
        "Le dataset contient une colonne `readmitted` (souvent: `NO`, `>30`, `<30`).\n",
        "- **Cible binaire** (simple): `1` si `<30`, sinon `0`.\n",
        "\n",
        "On fait aussi un nettoyage minimal des valeurs manquantes représentées par `?`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Normaliser les valeurs manquantes '?\n",
        "df = df.replace('?', np.nan)\n",
        "\n",
        "# Cible binaire\n",
        "TARGET_COL = 'readmitted'\n",
        "df = df.dropna(subset=[TARGET_COL])\n",
        "df['target_readmit_30'] = (df[TARGET_COL] == '<30').astype(int)\n",
        "\n",
        "print(df['target_readmit_30'].value_counts(normalize=True).rename('ratio'))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Sélection de features (MVP)\n",
        "Pour un capstone court, on part sur un sous-ensemble raisonnable (tu pourras élargir après)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sous-ensemble de colonnes courantes (ajuste selon ta version du dataset)\n",
        "candidate_cols = [\n",
        "    'time_in_hospital','num_lab_procedures','num_procedures','num_medications',\n",
        "    'number_outpatient','number_emergency','number_inpatient',\n",
        "    'diag_1','diag_2','diag_3',\n",
        "    'race','gender','age',\n",
        "    'admission_type_id','discharge_disposition_id','admission_source_id',\n",
        "    'insulin','change','diabetesMed'\n",
        "]\n",
        "available = [c for c in candidate_cols if c in df.columns]\n",
        "missing = [c for c in candidate_cols if c not in df.columns]\n",
        "print('Colonnes utilisées:', len(available))\n",
        "print('Colonnes manquantes (OK):', missing)\n",
        "\n",
        "X = df[available].copy()\n",
        "y = df['target_readmit_30'].copy()\n",
        "\n",
        "X.head()"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Visualisations (Data Processing)\n",
        "On regarde la distribution de la cible et 2–3 variables clés.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Distribution de la cible\n",
        "fig, ax = plt.subplots()\n",
        "y.value_counts().sort_index().plot(kind='bar', ax=ax)\n",
        "ax.set_title('Distribution de la cible (réadmission <30j)')\n",
        "ax.set_xlabel('Classe (0=non, 1=oui)')\n",
        "ax.set_ylabel('Nombre de cas')\n",
        "plt.show()\n",
        "\n",
        "# Exemple: time_in_hospital vs cible (si dispo)\n",
        "if 'time_in_hospital' in X.columns:\n",
        "    fig, ax = plt.subplots()\n",
        "    X.loc[y==0, 'time_in_hospital'].dropna().plot(kind='hist', bins=20, ax=ax, alpha=0.7, label='0')\n",
        "    X.loc[y==1, 'time_in_hospital'].dropna().plot(kind='hist', bins=20, ax=ax, alpha=0.7, label='1')\n",
        "    ax.set_title('time_in_hospital selon la cible')\n",
        "    ax.set_xlabel('Jours')\n",
        "    ax.legend()\n",
        "    plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Split train/test (avec stratification)\n",
        "Important pour préserver la proportion des classes si déséquilibrées."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
        ")\n",
        "print('Train:', X_train.shape, ' Test:', X_test.shape)\n",
        "print('y_train ratio:', y_train.mean().round(3), ' y_test ratio:', y_test.mean().round(3))\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Pipeline de preprocessing + modèles\n",
        "On utilise `ColumnTransformer` + `Pipeline` pour éviter le data leakage et garder un workflow reproductible.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Détecter types\n",
        "numeric_features = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
        "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
        "\n",
        "print('Num:', numeric_features)\n",
        "print('Cat:', categorical_features)\n",
        "\n",
        "# Preprocessing\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Modèles candidats\n",
        "candidates = {\n",
        "    'LogReg': LogisticRegression(max_iter=2000, random_state=RANDOM_STATE),\n",
        "    'RandomForest': RandomForestClassifier(n_estimators=300, random_state=RANDOM_STATE, n_jobs=-1),\n",
        "    'GradBoost': GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "scoring = {'acc':'accuracy','f1':'f1','roc_auc':'roc_auc'}\n",
        "\n",
        "results = []\n",
        "for name, model in candidates.items():\n",
        "    pipe = Pipeline([('preprocess', preprocessor), ('model', model)])\n",
        "    scores = cross_validate(pipe, X_train, y_train, cv=cv, scoring=scoring, n_jobs=-1)\n",
        "    results.append({\n",
        "        'model': name,\n",
        "        'cv_acc_mean': scores['test_acc'].mean(),\n",
        "        'cv_f1_mean': scores['test_f1'].mean(),\n",
        "        'cv_roc_auc_mean': scores['test_roc_auc'].mean(),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values('cv_f1_mean', ascending=False)\n",
        "results_df"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Hyperparameter tuning (1 modèle)\n",
        "On tune le meilleur modèle selon CV-F1 (tu peux changer le critère).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_name = results_df.iloc[0]['model']\n",
        "print('Best (selon CV-F1):', best_name)\n",
        "\n",
        "if best_name == 'RandomForest':\n",
        "    base = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1)\n",
        "    param_grid = {\n",
        "        'model__n_estimators': [200, 500],\n",
        "        'model__max_depth': [None, 10, 20],\n",
        "        'model__min_samples_split': [2, 5]\n",
        "    }\n",
        "elif best_name == 'LogReg':\n",
        "    base = LogisticRegression(max_iter=5000, random_state=RANDOM_STATE)\n",
        "    param_grid = {\n",
        "        'model__C': [0.1, 1.0, 10.0],\n",
        "        'model__penalty': ['l2'],\n",
        "        'model__solver': ['lbfgs']\n",
        "    }\n",
        "else:\n",
        "    base = GradientBoostingClassifier(random_state=RANDOM_STATE)\n",
        "    param_grid = {\n",
        "        'model__n_estimators': [100, 300],\n",
        "        'model__learning_rate': [0.05, 0.1],\n",
        "        'model__max_depth': [2, 3]\n",
        "    }\n",
        "\n",
        "pipe = Pipeline([('preprocess', preprocessor), ('model', base)])\n",
        "gs = GridSearchCV(pipe, param_grid=param_grid, scoring='f1', cv=cv, n_jobs=-1, verbose=1)\n",
        "gs.fit(X_train, y_train)\n",
        "\n",
        "print('Best params:', gs.best_params_)\n",
        "print('Best CV F1:', gs.best_score_)\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Évaluation finale sur le test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "best_pipe = gs.best_estimator_\n",
        "y_pred = best_pipe.predict(X_test)\n",
        "y_proba = None\n",
        "if hasattr(best_pipe.named_steps['model'], 'predict_proba'):\n",
        "    y_proba = best_pipe.predict_proba(X_test)[:,1]\n",
        "\n",
        "print(classification_report(y_test, y_pred, digits=3))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()\n",
        "\n",
        "if y_proba is not None:\n",
        "    print('ROC-AUC:', roc_auc_score(y_test, y_proba).round(3))\n",
        "    RocCurveDisplay.from_predictions(y_test, y_proba)\n",
        "    plt.show()\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11) Interprétabilité (simple)\n",
        "- Pour RandomForest/GB: importance des features (approx)\n",
        "- Pour LogReg: coefficients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "model = best_pipe.named_steps['model']\n",
        "\n",
        "# Récupérer les noms des features après OneHot (approx)\n",
        "ohe = best_pipe.named_steps['preprocess'].named_transformers_['cat'].named_steps['onehot'] if categorical_features else None\n",
        "cat_names = []\n",
        "if ohe is not None and len(categorical_features)>0:\n",
        "    cat_names = list(ohe.get_feature_names_out(categorical_features))\n",
        "feature_names = numeric_features + cat_names\n",
        "\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    importances = model.feature_importances_\n",
        "    imp = pd.DataFrame({'feature': feature_names, 'importance': importances}).sort_values('importance', ascending=False).head(20)\n",
        "    display(imp)\n",
        "elif hasattr(model, 'coef_'):\n",
        "    coefs = model.coef_.ravel()\n",
        "    coef_df = pd.DataFrame({'feature': feature_names, 'coef': coefs}).sort_values('coef', key=lambda s: np.abs(s), ascending=False).head(20)\n",
        "    display(coef_df)\n",
        "else:\n",
        "    print('Interprétabilité: non disponible pour ce modèle.')\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 12) Generative AI – résumé exécutif automatique\n",
        "Idée simple (et facile à justifier): générer un résumé à partir des métriques + top features.\n",
        "\n",
        "⚠️ Bonnes pratiques: donner du **contexte**, demander une **sortie structurée**, et interdire d’inventer des infos non présentes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def build_prompt(metrics: dict, top_features: pd.DataFrame) -> str:\n",
        "    top_txt = top_features.to_csv(index=False)\n",
        "    return f\"\"\"You are a healthcare data analyst.\n",
        "Write an executive summary (max 180 words) of this readmission risk model.\n",
        "\n",
        "Constraints:\n",
        "- Use ONLY the information provided below.\n",
        "- If something is missing, explicitly say it is unknown.\n",
        "- Output format:\n",
        "  1) Problem (1 sentence)\n",
        "  2) Results (bullets)\n",
        "  3) Top drivers (bullets)\n",
        "  4) Limitations (bullets)\n",
        "\n",
        "Metrics:\n",
        "{metrics}\n",
        "\n",
        "Top features:\n",
        "{top_txt}\n",
        "\"\"\" \n",
        "\n",
        "# Exemple de contenu à passer au LLM\n",
        "metrics_payload = {\n",
        "    'cv_best_f1': float(gs.best_score_),\n",
        "    'test_report': classification_report(y_test, y_pred, output_dict=True)\n",
        "}\n",
        "\n",
        "# Choisir table de drivers selon ce que tu as (imp ou coef)\n",
        "drivers = None\n",
        "if 'imp' in globals():\n",
        "    drivers = imp\n",
        "elif 'coef_df' in globals():\n",
        "    drivers = coef_df\n",
        "else:\n",
        "    drivers = pd.DataFrame({'feature':[], 'importance_or_coef':[]})\n",
        "\n",
        "prompt = build_prompt(metrics_payload, drivers.head(10))\n",
        "print(prompt[:800])\n",
        "\n",
        "# ---- Appel LLM (optionnel) ----\n",
        "# 1) mets ta clé dans une variable d’environnement OPENAI_API_KEY\n",
        "# 2) utilise l’API OpenAI / ou un autre LLM\n",
        "# Ici, on laisse volontairement un STUB pour éviter de bloquer ton exécution.\n"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 13) Model Card (document de transparence)\n",
        "Tu peux générer un fichier `MODEL_CARD.md` dans ton repo (ou le coller dans ton rapport)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from datetime import date\n",
        "\n",
        "model_card_md = f\"\"\"# Model Card – Readmission Risk Classifier\n",
        "\n",
        "## Model details\n",
        "- Date: {date.today()}\n",
        "- Task: binary classification (readmitted in <30 days)\n",
        "- Model: {type(best_pipe.named_steps['model']).__name__}\n",
        "\n",
        "## Intended use\n",
        "- Support early identification of higher-risk readmissions.\n",
        "- **Not** for clinical decision-making without human review.\n",
        "\n",
        "## Data\n",
        "- Dataset: Diabetes 130-US hospitals (1999–2008), public.\n",
        "- Unit: each row = inpatient encounter.\n",
        "\n",
        "## Metrics\n",
        "- Best CV F1: {gs.best_score_:.3f}\n",
        "- Test-set report: see notebook output.\n",
        "\n",
        "## Ethical / bias considerations\n",
        "- Potential bias by demographics (race, gender, age buckets).\n",
        "- Evaluate subgroup performance before deployment.\n",
        "\n",
        "## Limitations\n",
        "- Labels are historical; practice patterns may have changed.\n",
        "- Missing data and coding conventions can affect results.\n",
        "- Feature importance ≠ causality.\n",
        "\"\"\"\n",
        "\n",
        "print(model_card_md[:800])\n",
        "\n",
        "# Option: sauvegarder dans ton repo\n",
        "# with open('MODEL_CARD.md','w',encoding='utf-8') as f:\n",
        "#     f.write(model_card_md)\n"
      ],
      "outputs": [],
      "execution_count": null
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}